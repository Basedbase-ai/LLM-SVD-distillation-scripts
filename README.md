MoE-SVD-Distill: High-Performance Large Language Model Distillation
This project provides a high-performance, multi-GPU Python script for distilling knowledge from a large "teacher" Large Language Model (LLM) into a smaller "student" LLM. It leverages Singular Value Decomposition (SVD) and advanced techniques for handling Mixture-of-Experts (MoE) layers to create a high-quality LoRA (Low-Rank Adaptation) adapter. This allows the student model to mimic the teacher's capabilities without the need for full fine-tuning.
Key Features
Multi-GPU Support: Utilizes PyTorch's multiprocessing to significantly speed up the distillation process by distributing the workload across multiple GPUs.
SVD-based Distillation: Employs Singular Value Decomposition (SVD) to project the teacher model's weights into the student's smaller parameter space, preserving maximal information.
Mixture-of-Experts (MoE) Distillation: Implements a sophisticated method for distilling MoE layers. This involves creating "fingerprints" of teacher experts, clustering them using KMeans, and then generating synthetic student experts based on these clusters.
LoRA Adapter Generation: The script calculates the difference between the student model's original weights and the newly generated synthetic weights, then extracts LoRA A and B matrices from this difference.
Non-Destructive Workflow: For safety and verification, the script is designed to never delete the temporary files generated by each GPU worker. This gives you full control to inspect the intermediate results before consolidating them.
Flexible Configuration: Easily configure model paths, layer and expert counts, LoRA rank and alpha, and the number of GPUs to use, all from a centralized configuration block.
How It Works
The script follows a methodical process to distill the teacher model into a LoRA adapter for the student model.
Initialization: The script begins by reading the configuration parameters and loading the student model's weight map to divide the distillation tasks among the available GPUs.
Multi-GPU Worker Spawn: It spawns a separate process for each GPU. Each worker is assigned a subset of the student model's tensors to process.
Layer-to-Layer Mapping: For each layer in the student model, a corresponding layer or a weighted average of two adjacent layers from the teacher model is identified. This is based on the ratio of teacher layers to student layers.
Distillation of Standard Layers: For standard (non-MoE) layers, the script performs the following steps:
It interpolates the corresponding teacher layer tensors to create a "blended" teacher tensor.
This blended tensor is then projected into the student tensor's shape using SVD. A Fourier-based projection is used as a fallback in case of SVD errors.
The difference between the projected teacher tensor and the original student tensor is calculated.
Finally, LoRA A and B matrices are extracted from this difference tensor using SVD.
Distillation of MoE Layers: MoE layers are handled with a special process:
Fingerprinting: For each expert in the relevant teacher layers, a "fingerprint" is created by concatenating its weights. These fingerprints are then interpolated.
Clustering: The interpolated fingerprints of the teacher experts are clustered using KMeans to group similar experts. The number of clusters is equal to the number of experts in the student model's MoE layers.
Synthetic Expert Generation: For each student expert, the script identifies the teacher experts assigned to its cluster. The weights of these teacher experts are blended and projected to create a synthetic student expert.
LoRA Extraction: As with standard layers, the difference between the synthetic expert and the original student expert is used to generate LoRA weights.
Temporary File Storage: Each GPU worker saves its generated LoRA weights to a separate temporary .safetensors file.
Consolidation: After all workers have completed their tasks, the main process loads the LoRA weights from all the temporary files and consolidates them into a single, final LoRA adapter file.
Adapter Configuration: A corresponding adapter_config.json file is generated, which is necessary for loading the LoRA adapter with libraries like PEFT (Parameter-Efficient Fine-Tuning).
Requirements
To use this script, you will need the following Python libraries:
torch
torch.fft
safetensors
tqdm
numpy
scikit-learn
You can typically install these using pip:
code
Bash
pip install torch safetensors tqdm numpy scikit-learn
Configuration
All the necessary configurations are located in the CONFIGURATION section of the script.
TEACHER_MODEL_FOLDER: The file path to the directory containing the teacher model.
STUDENT_BASE_FOLDER: The file path to the directory containing the student model.
OUTPUT_LORA_PATH: The desired file path for the final, consolidated LoRA weights.
OUTPUT_LORA_CONFIG_PATH: The desired file path for the LoRA adapter configuration JSON file.
MODEL_ARCHITECTURE_CONFIG: A dictionary specifying the number of layers and experts per layer for both the teacher and student models.
RANK_MAP: A dictionary to define the rank of the LoRA matrices for different types of layers. A higher rank can capture more information but results in a larger file size.
LORA_ALPHA: The LoRA scaling factor.
NUM_GPUS: The number of GPUs you wish to utilize for the distillation process.
Usage
Configure the script: Open the LLM_distill_multi_gpu.py file and modify the variables in the CONFIGURATION section to match your models and desired output paths.
Run the script: Execute the script from your terminal:
code
Bash
python LLM_distill_multi_gpu.py
Monitor the process: The script will print progress updates for each GPU worker.
Manual Cleanup: After the script completes, the temporary worker files (temp_lora_weights_*.safetensors) will remain in the directory. Once you have verified that the final LoRA file has been created successfully, you can manually delete these temporary files.
Disclaimer
This is an advanced script intended for users with a good understanding of LLM architectures and the concepts of model distillation and LoRA. The high LoRA ranks used in the default configuration are intentional and are designed to capture a significant amount of information from the teacher model. Experimentation with different ranks and configurations may be necessary to achieve optimal results for your specific models.
